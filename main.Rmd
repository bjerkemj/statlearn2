---
title: 'Compulsory Exercise 2: Heart Diesease Inference'
subtitle: TMA4268 Statistical Learning V2023
author: | 
  | Torbj√∏rn Vatne, Ludvik Braathen and Johan Bjerkem
  | Group 11
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: 
- \usepackage{amsmath}
- \usepackage{makecell}

output:
  # html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: |
  The main goal of this assignment is gaining a deeper insight into what causes heart disease. We are curious to whether some simple measurements on the body metrics can give probable cause to suspect sickness. Our dataset a merger of five smaller data sets from Europe and North America. We investigate the data using plots and try to gain key insights on the predictors and their relationships. We mainly use histograms and bar plots, but try to use more advanced tools where it's appropriate. We find out that there are some inconsistencies within the predictors. The biggest error we find are that there are many cases of individuals with zero cholesterol (which is impossible). Since a lot of the cases with zero cholesterol were cases with heart disease, we decided to remove before we started training models on the data. We trained and tested our data on three models; logistic regression, knn and decision trees. The models all perform well on the accuracy metric. This seems to suggest that our hypothesis was right; it is possible to predict heart disease on simple body measurements.

---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
#require("knitr")
require("rmarkdown")
require("ggthemes") #For themes on the plot, as the theme_excel
require("cowplot") #For the plotgrid function which makes several plots in one image
require("GGally")
library(knitr)
library(kableExtra)
library(dplyr)
library(scales)
require("tree")

require("tidyverse")
require("rlang")
require("glmulti")

library(caret) # For K-fold CV
library(kknn) # For kNN
```

## Introduction: Scope and purpose of your project
Heart failure is one of the leading death causes in the world. Cardiovascular 
diseases which in many cases leads to heart failure, which stands for 
17.9 million deaths yearly. One third of people who suffer a heart attack or
a stroke die prematurely (under the age of 70). (https://www.who.int/health-topics/cardiovascular-diseases#tab=tab_1)

Detecting who is prone to heart failure is therefore an important task for 
society. First and foremost it's important because many lives can be saved by early
detection and implementing counter measures. Secondly this is a huge cost to
our society and a burden on our hospitals.

The problem we solve is classification with regards to inference. We are 
interested in the underlying relationships between the predictor, and which 
predictors that are more prevalent in patients with heart disease.

We will try to present our findings in an easy and understandable manner. This
is because we want this report to be accessible and simple for anyone to read. 
As heart disease is prevalent in all parts of society, therefore informing the
public of key indicators is important. As this is a small sample of the world 
population, we hope to find key insights that gives us a clue about
the health issues globally.

We found our dataset on *kaggle* (https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).
It's a combination of 5 datasets which makes it on of the biggest freely 
available datasets on **heart disease**.

## Descriptive data analysis/statistics
In this part we have chosen to hide a lot of the code. This is because it's 
mostly boiler plate for showing graphs and plots. Feel free to inspect the code
in the .Rmd file that should be attached.

### Load data
```{r, echo = TRUE, results='hide'}
heart <- read.csv("./heart.csv", header = T, stringsAsFactors = T)
str(heart)
summary(heart)
heart$FastingBS <- factor(heart$FastingBS, levels = c(0, 1), labels = c("No", "Yes"))
heart$HeartDisease <- factor(heart$HeartDisease, levels = c(0, 1), labels = c("No", "Yes"))
kable(head(heart), format = "markdown", align = "c", row.names = FALSE)
```
```{r, echo=FALSE}
truncate_colnames <- function(df, max_length = 10) {
  colnames(df) <- sapply(colnames(df), function(x) {
    if (nchar(x) > max_length) {
      paste0(substr(x, 1, max_length - 1), ".")
    } else {
      x
    }
  })
  return(df)
}

data <- heart
first_5_rows <- head(data, 5)

first_5_rows <- truncate_colnames(first_5_rows, max_length = 7)

kable(first_5_rows, format = "markdown", align = "c", row.names = FALSE)
```

We import the data set from file and immediately turn all columns with string values
into factors. After this we get a overview of the data 
using the *str* and *summary* methods. Here we notice that the values 
**fastingBS** and **heartDisease** need to be turned into factors. Lastly we
have a look at the head of the data.


#### Histograms

```{r histogram, fig.width=6, echo=FALSE}
hist_age <- ggplot(data = heart, mapping = aes(x = Age)) +
    geom_histogram(binwidth = 2, fill = "red", color = "black") +
    xlab("Age") +
    ylab("Frequency") +
    theme_minimal()

hist_restbp <- ggplot(data = heart, mapping = aes(x = RestingBP)) +
    geom_histogram(binwidth = 8, fill = "blue", color = "black") +
    xlab("RestingBP") +
    ylab("Frequency") +
    theme_minimal()

hist_chol <- ggplot(data = heart, mapping = aes(x = Cholesterol)) +
    geom_histogram(binwidth = 15, fill = "green", color = "black") +
    xlab("Cholesterol") +
    ylab("Frequency") +
    theme_minimal()

hist_maxhr <- ggplot(data=heart, mapping=aes(MaxHR)) +
  geom_histogram(binwidth = 10, fill="pink", color = "black") +
  xlab("Max HR") +
  ylab("Frequency") +
  theme_minimal()

plot_grid(hist_age, hist_restbp, hist_chol, hist_maxhr, nrow=2, ncol=2)
```
The **age** histogram seems legit, and look to be normally distributed with no unexpected values. Both **resting blood pressure** and **cholesterol** seem to have some fishy 0 values. To our knowledge and research these values are impossible, and seems to be a problem with the data set. Continuing, the **max hr** plot also look okay, with no outliers.

#### Percentage of heart disease cases vs. not in dataset.

```{r percentages}
classification_counts = table(heart$HeartDisease)
classification_percentages <- classification_counts / sum(classification_counts) * 100
barplot(classification_percentages, xlab = "Heart disease", ylab = "Percentages")
```
It's important to realize that this distribution does not reflect real life, and therefore a model's performance on new data may be poor, as the model has learned the patterns specific to the training data. It's important to be aware of this, as it may lead to inference bias. However, as we see in the **Cholesterol** vs. **Heart Disease**, most of the **zero-cholesterol** observations are also **heart disease** observations, so removing these will somewhat adjust the imbalanced distribution. Have a look below:

```{r updatedPercentages, echo=FALSE}

heart_filtered <- heart[heart$Cholesterol != 0, ]
classification_counts = table(heart_filtered$HeartDisease)
classification_percentages <- classification_counts / sum(classification_counts) * 100
barplot(classification_percentages, xlab = "Heart disease", ylab = "Percentages")
```


#### Correlations continous variables
```{r violin, fig.width=6, echo=FALSE}
hd_age <- ggplot(heart, aes(x = HeartDisease, y = Age, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "Age") +
  theme_minimal()

hd_op <- ggplot(heart, aes(x = HeartDisease, y = Oldpeak, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "Oldpeak") +
  theme_minimal()

hd_chol <- ggplot(heart, aes(x = HeartDisease, y = Cholesterol, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "Cholesterol") +
  theme_minimal()

hd_maxhr <- ggplot(heart, aes(x = HeartDisease, y = MaxHR, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "MaxHR") +
  theme_minimal()

plot_grid(hd_age, hd_op, hd_chol, hd_maxhr, nrow=2, ncol=2)
```

People with **heart diseases** are significantly older on average. This is not too surprising, as older people are more prone to **heart diseases** as the human body gets worn out over time. There is also sufficient evidence that values far from 0 in **old peak** suggests **heart disease**. As mentioned earlier there is a lot of errors in the **cholesterol** column. We can tell that a majority of the errors are linked to individuals with **heart disease**. This is an interesting curiosity with this specific dataset. Lastly we can see that lower **maxHR** seems to indicate **heart disease**, which we will comment more on in the next section.

#### Max Heart Rate and Age
```{r age_maxhr, echo=FALSE}
ggplot(heart, aes(x = Age, y = MaxHR)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "blue") +
  labs(x = "Age",
       y = "MaxHR") +
  theme_minimal()

correlation_test <- cor.test(heart$Age, heart$MaxHR, method = "pearson")

# Print the correlation coefficient and p-value
cat("Correlation coefficient:", correlation_test$estimate, "\nP-value:", correlation_test$p.value)
```
There's a correlation between **age** and **maxHR**, which makes sence since **MaxHR** 
generally decreases as a person gets older. This partially explains that people 
with **heart diseases** generally have a lower **maxHR**, however it might also be that
people with **heart diseases** generally are less exposed to anaerobic training, 
which usually helps **maxHR** stay higher.

```{r corr_matrix, echo=FALSE, eval=FALSE}
ggcorr(heart)
```

#### Categorical variables
```{r cat_variables, fig.width=8, echo=FALSE}
categorical_plot <- function(df, cat_var1, cat_var2) {
  cat_var1_sym <- rlang::sym(cat_var1)
  cat_var2_sym <- rlang::sym(cat_var2)
  
  df_prop <- df %>%
    count(!!cat_var1_sym, !!cat_var2_sym) %>%
    group_by(!!cat_var2_sym) %>%
    mutate(Percentage = n / sum(n) * 100)
  
  plot <- ggplot(df_prop, aes(x = !!cat_var2_sym, y = Percentage, fill = !!cat_var1_sym)) +
    geom_bar(stat = "identity", position = "fill") +
    labs(x = cat_var2) +
    theme_minimal() +
    theme(legend.title = element_text(size = 8),
          legend.text = element_text(size = 7),
          axis.title = element_text(size = 8),
          axis.text = element_text(size = 8)) +
    scale_y_continuous(labels = percent_format())
  
  return(plot)
}

hd_sex <- categorical_plot(heart, "HeartDisease", "Sex")
hd_cpt <- categorical_plot(heart, "HeartDisease", "ChestPainType")
hd_fastingBS <- categorical_plot(heart, "HeartDisease", "FastingBS")
hd_ECG <- categorical_plot(heart, "HeartDisease", "RestingECG")
hd_exAng <- categorical_plot(heart, "HeartDisease", "ExerciseAngina")
hd_ST <- categorical_plot(heart, "HeartDisease", "ST_Slope")
cpt_sex <- categorical_plot(heart, "ChestPainType", "Sex")

plot_grid(hd_sex, hd_cpt, cpt_sex, hd_fastingBS, hd_exAng, hd_ST, nrow=2, ncol=3)
```
In the first plot, we see that men are more likely to have a **heart disease** than women. We also see from the second plot that type of chest pain has a significant correlation with **heart diseases**. It is important to realize that many of the variables are correlated. For example, there are more asymptomatic men than women, the ChestPainType-category that is most likely for a person with a **heart disease**. As we see, ChestPainType depends on Sex. It is more difficult to see how resting electrocardiogram results predicts **heart disease** as all three categories are rather close in percentage of **heart disease** cases.

To summarize, many of the categorical variables seem to be correlated to **heart disease** and might therefore be valuable predictors, however one has to be aware that many of the variables are correlated within.


## Data preprocessing

### Remove zero-cholesterol observations and split data into train and test.
```{r dataprocessing}
set.seed(1)
train_perc <- 0.60
heart <- heart[heart$Cholesterol != 0, ]
train_index <- sample(1:nrow(heart), nrow(heart)*train_perc)
test_index <- (-train_index)
train <- heart[train_index, ]
test <- heart[test_index, ]
```

From the data analysis, we concluded that are some false **cholesterol** observations. There are several approaches to deal with missing data. Since there were many observations missing **cholesterol** data in this dataset, and the correct observations seemed close to normally distributed, we decided to drop the zero **cholesterol** observations. We decided to alter the values using KNN but it seemed fit to just drop them.


## Methods
### Logistic Regression
```{r glm}
glm.fit <- glm(HeartDisease ~ ., data = train, family = binomial)
# summary(glm.fit)

glm.probs <- predict(glm.fit, test, type = "response")

glm.pred <- rep(0, length(glm.probs))
glm.pred[glm.probs > .5] <- 1

glm.fit.confusion_matrix <- table(glm.pred, test$HeartDisease)
glm.fit.accuracy <- sum(diag(glm.fit.confusion_matrix)) / sum(glm.fit.confusion_matrix)
```

```{r best_subset_logreg}
glmulti.logistic.out <-
    glmulti(HeartDisease ~ ., data = heart,
            level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 1,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # glm function
            family = binomial)       # binomial family for logistic regression
## Show 5 best models (Use @ instead of $ for an S4 object)
glmulti.best.subset <- glmulti.logistic.out@formulas
#summary(glmulti.logistic.out@objects[[1]])

```

```{r best_model_logreg}
glm.best <- glm(HeartDisease ~ Sex + ChestPainType + ExerciseAngina + ST_Slope + 
    Age + Oldpeak + RestingBP, data = train, family = binomial)

glm.probs <- predict(glm.best, test, type = "response")

glm.pred <- rep(0, length(glm.probs))
glm.pred[glm.probs > .5] <- 1

glm.best.confusion_matrix <- table(glm.pred, test$HeartDisease)
glm.best.accuracy <- sum(diag(glm.best.confusion_matrix)) / sum(glm.best.confusion_matrix)

#car::Anova(glm.best)
```

Our evaluation metric is accuracy, which we will use for the next models as well. This can be calculated using the formula:

$$
\text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{True Positives} + \text{True Negatives} + \text{False Positives} + \text{False Negatives}}
$$

Logistic regression is used in classification problems where the output is of two states, in our case **heart disease**. Logistic regression utilizes the logistic function (AKA sigmoid function). The logistic function transforms a linear model to a probability between 0 and 1 which is useful in classification. One of the disadvantages with logistic regression is the assumption that the logit transformed probabilities and the predictors. In our case this assumption seems to hold fairly good. 

### K-fold CV on logistic regression

### 10-fold Cross Validation on Logistic regression
```{r logreg-CV}
train_control <- trainControl(method = "cv", number = 10)

logit_model <- train(HeartDisease ~ ., data = heart, method = "glm",
                     family = "binomial",
                     trControl = train_control,
                     preProcess = c("center", "scale"))

logit_model_red <- train(HeartDisease ~ Sex + ChestPainType + ExerciseAngina + ST_Slope + 
    Age, data = heart, method = "glm",
                     family = "binomial",
                     trControl = train_control,
                     preProcess = c("center", "scale"))

print(logit_model)
print(logit_model_red)
```


### k-Nearest Neighbors
```{r KNN}
test_y <- test$HeartDisease

knn_pred <- function(k_value){
  knn_model <- kknn(HeartDisease ~ ., train = train, test = test, k = k_value, scale = TRUE)
  knn_pred <- knn_model$fitted.values
  confusion_matrix <- table(Predicted = knn_pred, True = test_y)
  return(confusion_matrix)
}

k_values = 1:15
accuracies = rep(0,15)
for (k in k_values){
  confusion_matrix <- knn_pred(k)
  accuracies[k] <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
}
```
In kNN, the model calculates the distance from the new observation to each observation in the dataset, and then finds the k nearest and uses the most common class of the neighbors to classify the new observation. A limitation of kNN is it's computational complexity as it has to do n^2 computations for each new prediction, instead of creating an explicit model like logistic regression does.

### Trees
```{r}
library(tree)
tree.fit <- tree(HeartDisease ~ ., data = train)
#summary(tree.fit)

tree.pred <- predict(tree.fit, test, type = "class")
tree.confusion_matrix <- table(tree.pred, test$HeartDisease)
tree.accuracy <- sum(diag(tree.confusion_matrix)) / sum(tree.confusion_matrix)
```

A decision tree is a tree-based model used for both classification and regression tasks. It consists of internal nodes, branches, and leaf nodes. Each internal node represents a decision point based on a predictor value, and the branches stemming from these nodes represent different outcomes of that decision. As you traverse the tree from the top to bottom, you follow a path of choices that corresponds to a specific combination of predictor values. The leaf value at the bottom gives the probability. Due to the probabilities lying the leaf nodes the output is often step-like, which is characteristic of the model. It is not easy to decide on the depth of the tree for best output. A deep tree results in a drawback, but a short tree leads to over generalization. This means it is often difficult to manually find a suitable height for the given data. 

## Results and interpretation

Firstly, we will consider the results of logstic regression. We started with a model containing all predictors. The model obtained an accuracy of `r round(100*glm.fit.accuracy, 2)`%. We think this is a good result, but we were keen to further investigate this model and try to make improvements upon it. Below is the confusion matrix presented:

```{r}
glm.fit.confusion_matrix
```

Doing statistics on diseases we are often more concerned for the amount of false negatives than the accuracy. This is because it's a better solution to classify more people ass sick and don't miss any individuals and rather have them take a second test. 

In statistics, when studying diseases, it is often more crucial to minimize the number of false negatives than to simply focus on overall accuracy. This is because the cost of misclassifying a sick person as healthy (a false negative) can be much higher than misclassifying a healthy person as sick (a false positive). Therefore it is preferable to stay on the side of caution and classify more individuals as potentially sick, even if some of them are actually healthy. This aspect is of course more relevant to classification, but we thought it might be interesting to mention. In the top right corner of the confusion matrix we can tell that 13 has been classified as healthy, but in fact were sick.

To improve our logistic regression method we decided to apply best subset regression. We thought it would be a better choice than forward or backward selection in this case, since we have such few predictors. This gave a model with these predictors:

```{r}
glmulti.best.subset
```

One interesting thing to note here is that **maxHR** has been left out, which is probably due to it's negative correlation with **age**. This model prediction of `r round(100*glm.best.accuracy, 2)`% which is slightly better than the first model. But looking at the confusion matrix below we see that one more person been classified as false negative. In the methos section you can see we ran cross validation on the two model and the accuracy was basicly the same. In that case the latter model is preferred because it is smaller. One advantage of this is that it is easier to explain and understand the influence of each predictor.

```{r}
glm.best.confusion_matrix
```

Lastly we will look at at the decision tree model. It performed with an accuracy of `r round(100*tree.accuracy,2)`%, which is very close to the other models. The confusion matrix is presented below. Here we can see that it has a bigger amount of false negatives.

```{r}
tree.confusion_matrix
```


```{r knn_results}

plot(k_values, accuracies, main = "kNN over different k's", xlab = "k", ylab = "Accuracy")
confusion_matrix <- knn_pred(5)
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Total accuracy:", accuracy)
```
From the plot of kNN's accuracy on different k's, we found that the best value for k is 5, as increasing it to 10 or higher would likely lead to overfitting, specially since we didn't use cross validation to find the accuracies. With k=5, we obtained a total accuracy of `r round(100*accuracy,2)`%, and the misclassification rate of sick people was 14.17%.It is important to be aware that this rate might be lower than it would be if the dataset would have a realistic distribution of sick vs. not sick obserservations. 


## Summary

We have gained supporting evidence of our hypothesis that **heart disease** can be predicted with a high accuracy using simple body measurements. Some of the most important ones were **age**, **sex**, **chest pain type**, **exercise angina** and  **ST_Slope**. This also means we can predict who are prone to **heart diseases**. This can help extend the lives of millions of people and also save our hospitals from an unnecessary and expensive load of patients.
