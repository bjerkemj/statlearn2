---
title: 'Compulsory Exercise 2: Heart Diesease Inference'
subtitle: TMA4268 Statistical Learning V2023
author: | 
  | Torbjørn Vatne, Ludvik Braathen and Johan Bjerkem
  | Group 11
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: 
- \usepackage{amsmath}
- \usepackage{makecell}

output:
  # html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: |
  The main goal of this assignment is gaining a deeper insight into what causes heart disease. We are curious to whether some simple measurements on the body metrics can give probable cause to suspect sickness. Our dataset a merger of five smaller data sets from Europe and North America. We investigate the data using plots and try to gain key insights on the predictors and their relationships. We mainly use histograms and bar plots, but try to use more advanced tools where it's appropriate. We find out that there are some inconsistencies within the predictors. The biggest error we find are that there are many cases of individuals with zero cholesterol (which is impossible). Since a lot of the cases with zero cholesterol were cases with heart disease, we decided to remove before we started training models on the data. We trained and tested our data on three models; logistic regression, knn and decision trees. The models all perform well on the accuracy metric. This seems to suggest that our hypothesis was right; it is possible to predict heart disease on simple body measurements.

---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
#require("knitr")
require("rmarkdown")
require("ggthemes") #For themes on the plot, as the theme_excel
require("cowplot") #For the plotgrid function which makes several plots in one image
require("GGally")
library(knitr)
library(kableExtra)
library(dplyr)
library(scales)
require("tree")

require("tidyverse")
require("rlang")

library(MASS)  # For LDA and QDA
library(class) # For kNN
library(caret) # For K-fold CV
library(boot)  # For K-fold CV
```

## Introduction: Scope and purpose of your project
Heart failure is one of the leading death causes in the world. Cardiovascular 
diseases which in many cases leads to heart failure, which stands for 
17.9 million deaths yearly. One third of people who suffer a heart attack or
a stroke die prematurely (under the age of 70). (https://www.who.int/health-topics/cardiovascular-diseases#tab=tab_1)

Detecting who is prone to heart failure is therefore an important task for 
society. First and foremost it's important because many lives can be saved by early
detection and implementing counter measures. Secondly this is a huge cost to
our society and a burden on our hospitals.

The problem we solve is classification with regards to inference. We are 
interested in the underlying relationships between the predictor, and which 
predictors that are more prevalent in patients with heart disease.

We will try to present our findings in an easy and understandable manner. This
is because we want this report to be accessible and simple for anyone to read. 
As heart disease is prevalent in all parts of society, therefore informing the
public of key indicators is important. As this is a small sample of the world 
population, we hope to find key insights that gives us a clue about
the health issues globally.

We found our dataset on *kaggle* (https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).
It's a combination of 5 datasets which makes it on of the biggest freely 
available datasets on **heart disease**.

## Descriptive data analysis/statistics
In this part we have chosen to hide a lot of the code. This is because it's 
mostly boiler plate for showing graphs and plots. Feel free to inspect the code
in the .Rmd file that should be attached.

### Load data
```{r, echo = TRUE, results='hide'}
heart <- read.csv("./heart.csv", header = T, stringsAsFactors = T)
str(heart)
summary(heart)
heart$FastingBS <- factor(heart$FastingBS, levels = c(0, 1), labels = c("No", "Yes"))
heart$HeartDisease <- factor(heart$HeartDisease, levels = c(0, 1), labels = c("No", "Yes"))
kable(head(heart), format = "markdown", align = "c", row.names = FALSE)
```
```{r, echo=FALSE}
truncate_colnames <- function(df, max_length = 10) {
  colnames(df) <- sapply(colnames(df), function(x) {
    if (nchar(x) > max_length) {
      paste0(substr(x, 1, max_length - 1), ".")
    } else {
      x
    }
  })
  return(df)
}

data <- heart
first_5_rows <- head(data, 5)

first_5_rows <- truncate_colnames(first_5_rows, max_length = 7)

kable(first_5_rows, format = "markdown", align = "c", row.names = FALSE)
```

We import the data set from file and immediately turn all columns with string values
into factors. After this we get a overview of the data 
using the *str* and *summary* methods. Here we notice that the values 
**fastingBS** and **heartDisease** need to be turned into factors. Lastly we
have a look at the head of the data.


#### Histograms

```{r histogram, fig.width=6, echo=FALSE}
hist_age <- ggplot(data = heart, mapping = aes(x = Age)) +
    geom_histogram(binwidth = 2, fill = "red", color = "black") +
    xlab("Age") +
    ylab("Frequency") +
    theme_minimal()

hist_restbp <- ggplot(data = heart, mapping = aes(x = RestingBP)) +
    geom_histogram(binwidth = 8, fill = "blue", color = "black") +
    xlab("RestingBP") +
    ylab("Frequency") +
    theme_minimal()

hist_chol <- ggplot(data = heart, mapping = aes(x = Cholesterol)) +
    geom_histogram(binwidth = 15, fill = "green", color = "black") +
    xlab("Cholesterol") +
    ylab("Frequency") +
    theme_minimal()

hist_maxhr <- ggplot(data=heart, mapping=aes(MaxHR)) +
  geom_histogram(binwidth = 10, fill="pink", color = "black") +
  xlab("Max HR") +
  ylab("Frequency") +
  theme_minimal()

plot_grid(hist_age, hist_restbp, hist_chol, hist_maxhr, nrow=2, ncol=2)
```
The **age** histogram seems legit, and look to be normally distributed with no unexpected values. Both **resting blood pressure** and **cholesterol** seem to have some fishy 0 values. To our knowledge and research these values are impossible, and seems to be a problem with the data set. Continuing, the **max hr** plot also look okay, with no outliers.

#### Percentage of heart disease cases vs. not in dataset.

```{r percentages}
classification_counts = table(heart$HeartDisease)
classification_percentages <- classification_counts / sum(classification_counts) * 100
barplot(classification_percentages, xlab = "Heart disease", ylab = "Percentages")
```
It's important to realize that this distribution does not reflect real life, and therefore a model's performance on new data may be poor, as the model has learned the patterns specific to the training data. It's important to be aware of this, as it may lead to inference bias. However, as we see in the **Cholesterol** vs. **Heart Disease**, most of the **zero-cholesterol** observations are also **heart disease** observations, so removing these will somewhat adjust the imbalanced distribution. Have a look below:

```{r updatedPercentages, echo=FALSE}

heart_filtered <- heart[heart$Cholesterol != 0, ]
classification_counts = table(heart_filtered$HeartDisease)
classification_percentages <- classification_counts / sum(classification_counts) * 100
barplot(classification_percentages, xlab = "Heart disease", ylab = "Percentages")
```


#### Correlations continous variables
```{r violin, fig.width=6, echo=FALSE}
hd_age <- ggplot(heart, aes(x = HeartDisease, y = Age, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "Age") +
  theme_minimal()

hd_op <- ggplot(heart, aes(x = HeartDisease, y = Oldpeak, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "Oldpeak") +
  theme_minimal()

hd_chol <- ggplot(heart, aes(x = HeartDisease, y = Cholesterol, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "Cholesterol") +
  theme_minimal()

hd_maxhr <- ggplot(heart, aes(x = HeartDisease, y = MaxHR, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "MaxHR") +
  theme_minimal()

plot_grid(hd_age, hd_op, hd_chol, hd_maxhr, nrow=2, ncol=2)
```

People with **heart diseases** are significantly older on average. This is not too surprising, as older people are more prone to **heart diseases** as the human body gets worn out over time. There is also sufficient evidence that values far from 0 in **old peak** suggests **heart disease**. As mentioned earlier there is a lot of errors in the **cholesterol** column. We can tell that a majority of the errors are linked to individuals with **heart disease**. This is an interesting curiosity with this specific dataset. Lastly we can see that lower **maxHR** seems to indicate **heart disease**, which we will comment more on in the next section.

#### Max Heart Rate and Age
```{r age_maxhr, echo=FALSE}
ggplot(heart, aes(x = Age, y = MaxHR)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "blue") +
  labs(x = "Age",
       y = "MaxHR") +
  theme_minimal()

correlation_test <- cor.test(heart$Age, heart$MaxHR, method = "pearson")

# Print the correlation coefficient and p-value
cat("Correlation coefficient:", correlation_test$estimate, "\nP-value:", correlation_test$p.value)
```
There's a correlation between **age** and **maxHR**, which makes sence since **MaxHR** 
generally decreases as a person gets older. This partially explains that people 
with **heart diseases** generally have a lower **maxHR**, however it might also be that
people with **heart diseases** generally are less exposed to anaerobic training, 
which usually helps **maxHR** stay higher.

```{r corr_matrix, echo=FALSE, eval=FALSE}
ggcorr(heart)
```

#### Categorical variables
```{r cat_variables, fig.width=8, echo=FALSE}
categorical_plot <- function(df, cat_var1, cat_var2) {
  cat_var1_sym <- rlang::sym(cat_var1)
  cat_var2_sym <- rlang::sym(cat_var2)
  
  df_prop <- df %>%
    count(!!cat_var1_sym, !!cat_var2_sym) %>%
    group_by(!!cat_var2_sym) %>%
    mutate(Percentage = n / sum(n) * 100)
  
  plot <- ggplot(df_prop, aes(x = !!cat_var2_sym, y = Percentage, fill = !!cat_var1_sym)) +
    geom_bar(stat = "identity", position = "fill") +
    labs(x = cat_var2) +
    theme_minimal() +
    theme(legend.title = element_text(size = 8),
          legend.text = element_text(size = 7),
          axis.title = element_text(size = 8),
          axis.text = element_text(size = 8)) +
    scale_y_continuous(labels = percent_format())
  
  return(plot)
}

hd_sex <- categorical_plot(heart, "HeartDisease", "Sex")
hd_cpt <- categorical_plot(heart, "HeartDisease", "ChestPainType")
hd_fastingBS <- categorical_plot(heart, "HeartDisease", "FastingBS")
hd_ECG <- categorical_plot(heart, "HeartDisease", "RestingECG")
hd_exAng <- categorical_plot(heart, "HeartDisease", "ExerciseAngina")
hd_ST <- categorical_plot(heart, "HeartDisease", "ST_Slope")
cpt_sex <- categorical_plot(heart, "ChestPainType", "Sex")

plot_grid(hd_sex, hd_cpt, cpt_sex, hd_fastingBS, hd_exAng, hd_ST, nrow=2, ncol=3)
```
In the first plot, we see that men are more likely to have a **heart disease** than women. We also see from the second plot that type of chest pain has a significant correlation with **heart diseases**. It is important to realize that many of the variables are correlated. For example, there are more asymptomatic men than women, the ChestPainType-category that is most likely for a person with a **heart disease**. As we see, ChestPainType depends on Sex. It is more difficult to see how resting electrocardiogram results predicts **heart disease** as all three categories are rather close in percentage of **heart disease** cases.

To summarize, many of the categorical variables seem to be correlated to **heart disease** and might therefore be valuable predictors, however one has to be aware that many of the variables are correlated within.


## Data preprocessing

### Remove zero-cholesterol observations and split data into train and test.
```{r dataprocessing}
set.seed(1)
train_perc <- 0.60
heart <- heart[heart$Cholesterol != 0, ]
train_index <- sample(1:nrow(heart), nrow(heart)*train_perc)
test_index <- (-train_index)
train <- heart[train_index, ]
test <- heart[test_index, ]
```

From the data analysis, we concluded that are some false **cholesterol** observations. There are several approaches to deal with missing data. Since there were many observations missing **cholesterol** data in this dataset, and the correct observations seemed close to normally distributed, we decided to drop the zero **cholesterol** observations. We decided to alter the values using KNN but it seemed fit to just drop them.


## Methods
### Logistic Regression
```{r glm}
glm.fit <- glm(HeartDisease ~ ., data = train, family = binomial)
# summary(glm.fit)

glm.probs <- predict(glm.fit, test, type = "response")

glm.pred <- rep(0, length(glm.probs))
glm.pred[glm.probs > .5] <- 1

confusion_matrix <- table(glm.pred, test$HeartDisease)
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy)
```

```{r best_subset_logreg}
require("glmulti")

glmulti.logistic.out <-
    glmulti(HeartDisease ~ ., data = heart,
            level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 1,         # Keep 5 best models
            plotty = F, report = T,  # No plot or interim reports
            fitfunction = "glm",     # glm function
            family = binomial)       # binomial family for logistic regression
## Show 5 best models (Use @ instead of $ for an S4 object)
glmulti.logistic.out@formulas
summary(glmulti.logistic.out@objects[[1]])
```


```{r best_model_logreg}
glm.best <- glm(HeartDisease ~ Sex + ChestPainType + ExerciseAngina + ST_Slope + 
    Age + Oldpeak + RestingBP, data = train, family = binomial)

glm.probs <- predict(glm.best, test, type = "response")

glm.pred <- rep(0, length(glm.probs))
glm.pred[glm.probs > .5] <- 1

confusion_matrix <- table(glm.pred, test$HeartDisease)
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy)

car::Anova(glm.best)
```



Logistic regression gives a good estimation when we set the cutoff range at 0.5. Our evaluation metric is accuracy, which we will use for the next models as well. This can be calculated using the forumla:

$$
\text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{True Positives} + \text{True Negatives} + \text{False Positives} + \text{False Negatives}}
$$

Logistic regression is used in classification problems where the output is of two states, in our case **heart disease**. Logistic regression utilizes the logistic function (AKA sigmoid function). The logistic function transforms a linear model to a probability between 0 and 1 which is useful in classification. One of the disadvantages with logistic regression is the assumption that the logit transformed probabilities and the predictors. In our case this assumption seems to hold fairly good. 

### K-fold CV on logistic regression

```{r logreg-CV}
#k <- 10
glm.fit1 <- glm(HeartDisease ~ ., data = heart, family = binomial)
delta <- cv.glm(heart, glm.fit1, K=10)$delta
# Hva er tallene i delta? 1-Accuracy eller er det et annet tall?
```


### k-Nearest Neighbors
```{r KNN}
train_x <- train[, !(names(train) %in% "HeartDisease")]
train_y <- train$HeartDisease
test_x <- test[, !(names(test) %in% "HeartDisease")]
test_y <- test$HeartDisease

k_value <- 15
knn_model <- kknn(HeartDisease ~ ., train = train, test = test, k = k_value, scale = TRUE)

knn_pred <- knn_model$fitted.values

confusion_matrix <- table(Predicted = knn_pred, True = test_y)
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy)
```
In kNN, the model calculates the distance from the new observation to each observation in the dataset, and then finds the k nearest and uses the most common class of the neighbors to classify the new observation. A limitation of kNN is it's computational complexity as it has to do n^2 computations for each new prediction, instead of creating an explicit model like logistic regression does.


### Trees
```{r}
library(tree)
tree.fit <- tree(HeartDisease ~ ., data = train)
summary(tree.fit)

tree.pred <- predict(tree.fit, test, type = "class")
confusion_matrix <- table(tree.pred, test$HeartDisease)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy)
```

A decision tree is a tree-based model used for both classification and regression tasks. It consists of internal nodes, branches, and leaf nodes. Each internal node represents a decision point based on a predictor value, and the branches stemming from these nodes represent different outcomes of that decision. As you traverse the tree from the top to bottom, you follow a path of choices that corresponds to a specific combination of predictor values. The leaf value at the bottom gives the probability. Due to the probabilities lying the leaf nodes the output is often step-like, which is characteristic of the model. It is not easy to decide on the depth of the tree for best output. A deep tree results in a drawback, but a short tree leads to over generalization. This means it is often difficult to manually find a suitable height for the given data. 

## Results and interpretation
Ved liten prosentandel false negative (ift hvor mange syke det er) er dette bra for modellen, men det er viktig å vite at siden vårt dataset er 50/50 distribuert, kan denne prosentandelen være lavere enn den ville vært på en modell trent på et datasett som reflekterer real world.

- på den modellen vi velger å se videre på: 
  - kjør best subset selection. 
  - sjekk misclassification rate innenfor alle syke.


## Summary

We have gained supporting evidence of our hypothesis that **heart disease** can be predicted with a high accuracy using simple body measurements. Some of the most important ones was **age**, **sex**, **chest pain type**, **exercise angina** and  **ST_Slope**. This also means we can predict who are prone to **heart diseases**. This can help extend the lives of millions of people and also save our hospitals from an unnecessary and expensive load of patients.
