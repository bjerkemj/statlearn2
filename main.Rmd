---
title: 'Compulsory Exercise 2: Heart Diesease Inference'
subtitle: TMA4268 Statistical Learning V2023
author: | 
  | Torbj√∏rn Vatne, Ludvik Braathen and Johan Bjerkem
  | Group 11
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: 
- \usepackage{amsmath}
- \usepackage{makecell}

output:
  # html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: |
  TODO

---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
#require("knitr")
require("rmarkdown")
require("ggthemes") #For themes on the plot, as the theme_excel
require("cowplot") #For the plotgrid function which makes several plots in one image
require("GGally")
library(knitr)
library(kableExtra)
library(dplyr)
library(scales)
require("tree")

require("tidyverse")
require("rlang")

library(MASS)  # For LDA and QDA
library(kknn)  # For KNN
library(caret) # For K-fold CV
library(boot)  # For K-fold CV
```

## Introduction: Scope and purpose of your project
Heart failure is one of the leading death causes in the world. Cardiovascular 
diseases (CVDs) which in many cases leads to heart failure stands for 
17.9 million deaths yearly. One third of people who suffer a heart attack or
a stroke die prematurely (under the age of 70). (https://www.who.int/health-topics/cardiovascular-diseases#tab=tab_1)

Detecting who is prone to heart failure is therefore an important task for 
society. First and foremost it's important as lives can be saved by early
detection and implementing counter measures. Secondly this is a huge cost to
our society and a burden on our hospitals.

The problem we solve is classification with regards to inference. We are 
interested in the underlying relationsships between the predictor, and which 
predictors that are more prevelant in patients with heart disease.


We will try to present our findings in an easy and understandable manner. This
is because we want this report to be accessible and simple for anyone to read. 
As heart disease is prevalent in all parts of society, therefore informing the
public of key indicators is important.

We found our dataset on *kaggle* (https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).
It's a combination of 5 datasets which makes it on of the biggest freely 
available datasets on heart failure.

## Descriptive data analysis/statistics
In this part we have chosen to hide a lot of the code. This is because it's 
mostly boiler plate for showing graphs and plots.

### Load data
```{r, echo = TRUE, results='hide'}
heart <- read.csv("./heart.csv", header = T, stringsAsFactors = T)
str(heart)
summary(heart)
heart$FastingBS <- factor(heart$FastingBS, levels = c(0, 1), labels = c("No", "Yes"))
heart$HeartDisease <- factor(heart$HeartDisease, levels = c(0, 1), labels = c("No", "Yes"))
kable(head(heart), format = "markdown", align = "c", row.names = FALSE)
```
```{r, echo=FALSE}
truncate_colnames <- function(df, max_length = 10) {
  colnames(df) <- sapply(colnames(df), function(x) {
    if (nchar(x) > max_length) {
      paste0(substr(x, 1, max_length - 1), ".")
    } else {
      x
    }
  })
  return(df)
}

data <- heart
first_5_rows <- head(data, 5)

first_5_rows <- truncate_colnames(first_5_rows, max_length = 7)

kable(first_5_rows, format = "markdown", align = "c", row.names = FALSE)
```

We import the data set from file and immediately turn all columns with values
as strings into factors. After this we get a overview of the data 
using the *str* and *summary* methods. Here we notice that the values 
**fastingBS** and **heartDisease** need to be turned into factors. Lastly we
have a look at the head of the data.


#### Histograms

```{r histogram, fig.width=6, echo=FALSE}
hist_age <- ggplot(data = heart, mapping = aes(x = Age)) +
    geom_histogram(binwidth = 2, fill = "red", color = "black") +
    xlab("Age") +
    ylab("Frequency") +
    theme_minimal()

hist_restbp <- ggplot(data = heart, mapping = aes(x = RestingBP)) +
    geom_histogram(binwidth = 8, fill = "blue", color = "black") +
    xlab("RestingBP") +
    ylab("Frequency") +
    theme_minimal()

hist_chol <- ggplot(data = heart, mapping = aes(x = Cholesterol)) +
    geom_histogram(binwidth = 15, fill = "green", color = "black") +
    xlab("Cholesterol") +
    ylab("Frequency") +
    theme_minimal()

hist_maxhr <- ggplot(data=heart, mapping=aes(MaxHR)) +
  geom_histogram(binwidth = 10, fill="pink", color = "black") +
  xlab("Max HR") +
  ylab("Frequency") +
  theme_minimal()

plot_grid(hist_age, hist_restbp, hist_chol, hist_maxhr, nrow=2, ncol=2)
```
The **age** histogram seems legit and has no unexpected values. Both **resting 
blood pressure** and **cholesterol** seem to have some fishy 0 values. To our
knowledge and research these values are impossible, and seems to be a problem
with the data set. Continuing, the **max hr** plot also look okay, with no 
outliers.

#### Percentage of heart disease cases vs. not in dataset.

```{r percentages}
classification_counts = table(heart$HeartDisease)
classification_percentages <- classification_counts / sum(classification_counts) * 100
barplot(classification_percentages, xlab = "Heart disease", ylab = "Percentages")

```


#### Correlations continous variables
```{r violin, fig.width=6, echo=FALSE}
hd_age <- ggplot(heart, aes(x = HeartDisease, y = Age, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "Age") +
  theme_minimal()

hd_op <- ggplot(heart, aes(x = HeartDisease, y = Oldpeak, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "Oldpeak") +
  theme_minimal()

hd_chol <- ggplot(heart, aes(x = HeartDisease, y = Cholesterol, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "Cholesterol") +
  theme_minimal()

hd_maxhr <- ggplot(heart, aes(x = HeartDisease, y = MaxHR, fill = HeartDisease)) +
  geom_violin() +
  labs(x = "Heart Disease",
       y = "MaxHR") +
  theme_minimal()

plot_grid(hd_age, hd_op, hd_chol, hd_maxhr, nrow=2, ncol=2)
```

People with heart diseases are significantly older on average. This is not too
surprising, as older people are more prone to heart diseases as the human body
gets worn out over time. 

Bla bla bla, comment on the graphs above.

#### Max Heart Rate and Age
```{r age_maxhr, echo=FALSE}
ggplot(heart, aes(x = Age, y = MaxHR)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "blue") +
  labs(x = "Age",
       y = "MaxHR") +
  theme_minimal()

correlation_test <- cor.test(heart$Age, heart$MaxHR, method = "pearson")

# Print the correlation coefficient and p-value
cat("Correlation coefficient:", correlation_test$estimate, "\nP-value:", correlation_test$p.value)
```
There's a correlation between **age** and **maxHR**, which makes sence since **MaxHR** 
generally decreases as a person gets older. This partially explains that people 
with heart diseases generally have a lower **maxHR**, however it might also be that
people with heart diseases generally are less exposed to anaerobic training, 
which usually helps **maxHR** stay higher.


```{r corr_matrix}
ggcorr(heart)
```



#### Categorical variables
```{r cat_variables, fig.width=8, echo=FALSE}
categorical_plot <- function(df, cat_var1, cat_var2) {
  cat_var1_sym <- rlang::sym(cat_var1)
  cat_var2_sym <- rlang::sym(cat_var2)
  
  df_prop <- df %>%
    count(!!cat_var1_sym, !!cat_var2_sym) %>%
    group_by(!!cat_var2_sym) %>%
    mutate(Percentage = n / sum(n) * 100)
  
  plot <- ggplot(df_prop, aes(x = !!cat_var2_sym, y = Percentage, fill = !!cat_var1_sym)) +
    geom_bar(stat = "identity", position = "fill") +
    labs(x = cat_var2) +
    theme_minimal() +
    theme(legend.title = element_text(size = 8),
          legend.text = element_text(size = 7),
          axis.title = element_text(size = 8),
          axis.text = element_text(size = 8)) +
    scale_y_continuous(labels = percent_format())
  
  return(plot)
}

hd_sex <- categorical_plot(heart, "HeartDisease", "Sex")
hd_cpt <- categorical_plot(heart, "HeartDisease", "ChestPainType")
hd_fastingBS <- categorical_plot(heart, "HeartDisease", "FastingBS")
hd_ECG <- categorical_plot(heart, "HeartDisease", "RestingECG")
hd_exAng <- categorical_plot(heart, "HeartDisease", "ExerciseAngina")
hd_ST <- categorical_plot(heart, "HeartDisease", "ST_Slope")
#cpt_sex <- categorical_plot(heart, "ChestPainType", "Sex")

plot_grid(hd_sex, hd_cpt, hd_fastingBS, hd_ECG, hd_exAng, hd_ST, nrow=2, ncol=3)
```
In the first plot, we see that men are more likely to have a heart disease than women. We also see from the second plot that type of chest pain has a significant correlation with heart diseases. It is important to realize that many of the variables are correlated. For example, there are more asymptomatic men than women, the ChestPainType-category that is most likely for a person with a heart disease. As we see, ChestPainType depends on Sex. It is more difficult to see how resting electrocardiogram results predicts heart disease as all three categories are rather close in percentage of heart disease cases.

To summarize, many of the categorical variables seem to be correlated to heart disease and might therefore be valuable predictors, however one has to be aware that many of the variables are correlated within.


## Data preprocessing

### Remove zero-cholesterol observations
```{r dataprocessing}
set.seed(1)
train_perc <- 0.70
heart <- heart[heart$Cholesterol != 0, ]
train_index <- sample(1:nrow(heart), nrow(heart)*train_perc)
test_index <- (-train_index)
train <- heart[train_index, ]
test <- heart[test_index, ]
```
From the data analysis, we concluded that are some false cholesterol observations. There are several approaches to deal with missing data. Since there were many observations missing cholesterol-data in this dataset, and the correct observations seemed close to normally distributed, we decided to drop the zero-cholesterol observations. 

## Methods
### Logistic Regression
```{r glm}
glm.fit <- glm(HeartDisease ~ ., data = train, family = binomial)
# summary(glm.fit)

glm.probs <- predict(glm.fit, test, type = "response")

glm.pred <- rep(0, length(glm.probs))
glm.pred[glm.probs > .5] <- 1

confusion_matrix <- table(glm.pred, test$HeartDisease)
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy)
```
This model gives a good estimation when we set the cutoff range at 0.5.
```{r}
glm.fit2 <- glm(HeartDisease ~ . - Age - RestingBP - RestingECG - MaxHR, data = train, family = binomial)
# summary(glm.fit2)

glm.probs2 <- predict(glm.fit2, test, type = "response")

glm.pred2 <- rep(0, length(glm.probs2))
glm.pred2[glm.probs2 > .5] <- 1

confusion_matrix <- table(glm.pred2, test$HeartDisease)
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy)
```
By looking at the summary of the previous model we found out at that we could remove **age**, **restingBP**, **restingEXg** and **MaxHR**.

### K-fold CV on logistic regression
```{r logreg-CV}
k <- 10
glm.fit3 <- glm(HeartDisease ~ . - Age - RestingBP - RestingECG - MaxHR, data = heart, family = binomial)
cv.glm(heart, glm.fit3)$delta
```


## These can be tested: QDA, Naive bayes, KNN and Poisson. 


### Linear Discriminant Analysis
```{r LDA}
lda_model <- lda(HeartDisease ~ ., data = train)

print(lda_model)
lda_pred <- predict(lda_model, newdata = test)

confusion_matrix <- table(Predicted = lda_pred$class, True = test$HeartDisease)
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy)
```
The LDA is only fitted on continuous predictors as it does not directly handle 

### 10-fold CV on LDA
```{r LDA-CV}
k <- 10


```

### Quadratic discriminant analysis on continous predictors
```{r QDA}
qda_model <- qda(HeartDisease ~ ., data = train)

qda_pred <- predict(qda_model, newdata = test)

confusion_matrix <- table(Predicted = qda_pred$class, True = test$HeartDisease)
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy)
```


### KNN
```{r KNN}
train_x <- train[, !(names(train) %in% "HeartDisease")]
train_y <- train$HeartDisease
test_x <- test[, !(names(test) %in% "HeartDisease")]
test_y <- test$HeartDisease

k_value <- 15
knn_model <- kknn(HeartDisease ~ ., train = train, test = test, k = k_value, scale = TRUE)

knn_pred <- knn_model$fitted.values

confusion_matrix <- table(Predicted = knn_pred, True = test_y)
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy)
```


### Trees
```{r}
library(tree)
tree.fit <- tree(HeartDisease ~ ., data = train)
summary(tree.fit)

tree.pred <- predict(tree.fit, test, type = "class")
confusion_matrix <- table(tree.pred, test$HeartDisease)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy)
```
This performs almost as well as logistic regression.




## Results and interpretation


## Summary
